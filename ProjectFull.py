# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETYrrY3qp7WOyh6PVQRw_eWnGyJVi3sZ
"""

!pip install requests beautifulsoup4 transformers selenium
!apt-get update
!apt install chromium-chromedriver

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer
import sqlite3
import os

# Setup ChromeDriver options for headless execution
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# Initialize ChromeDriver (update path as per your environment)
chrome_driver_path = '/usr/lib/chromium-browser/chromedriver'
driver = webdriver.Chrome(executable_path=chrome_driver_path, options=chrome_options)

# Function to fetch web content using requests and BeautifulSoup
def fetch_web_content(url):
    response = requests.get(url)
    response.raise_for_status()
    return response.text

# Function to scrape content using BeautifulSoup
def scrape_content(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    paragraphs = soup.find_all('p')
    content = ' '.join([para.get_text() for para in paragraphs])
    return content

# Function to take screenshot using Selenium
def take_screenshot(url, filename):
    driver.get(url)
    driver.save_screenshot(filename)

# Function to generate content summary using Hugging Face's pipeline
def generate_summary(content):
    summarizer = pipeline("summarization")
    summary = summarizer(content, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
    return summary

# Function to initialize SQLite database
def initialize_database():
    conn = sqlite3.connect('workflow_results.db')
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS workflow_results (
            url TEXT PRIMARY KEY,
            scraped_content TEXT,
            llm_summary TEXT,
            screenshot_path TEXT
        )
    ''')
    conn.commit()
    conn.close()

# Function to store results in SQLite database
def store_results(url, scraped_content, llm_summary, screenshot_path):
    conn = sqlite3.connect('workflow_results.db')
    cursor = conn.cursor()
    cursor.execute('''
        INSERT OR REPLACE INTO workflow_results (url, scraped_content, llm_summary, screenshot_path)
        VALUES (?, ?, ?, ?)
    ''', (url, scraped_content, llm_summary, screenshot_path))
    conn.commit()
    conn.close()

# Function to fetch cached results from SQLite database
def fetch_cached_results(url):
    conn = sqlite3.connect('workflow_results.db')
    cursor = conn.cursor()
    cursor.execute('''
        SELECT scraped_content, llm_summary, screenshot_path FROM workflow_results WHERE url=?
    ''', (url,))
    row = cursor.fetchone()
    conn.close()
    return row

# Function to handle the entire workflow
def handle_workflow(url):
    # Check if results are cached
    cached_results = fetch_cached_results(url)
    if cached_results:
        scraped_content, llm_summary, screenshot_path = cached_results
        print("Results fetched from cache:")
    else:
        # Fetch web content and scrape using BeautifulSoup
        html_content = fetch_web_content(url)
        scraped_content = scrape_content(html_content)[:1024]  # Truncate to 1024 tokens

        # Take screenshot
        screenshot_filename = 'screenshot.png'
        take_screenshot(url, screenshot_filename)
        screenshot_path = os.path.abspath(screenshot_filename)

        # Generate LLM summary
        llm_summary = generate_summary(scraped_content)

        # Store results in database
        store_results(url, scraped_content, llm_summary, screenshot_path)
        print("Results generated and stored in database:")

    # Print results or further process as required
    print(f"Scraped Content:\n{scraped_content}\n")
    print(f"LLM Generated Summary:\n{llm_summary}\n")
    print(f"Screenshot Path: {screenshot_path}")

# Function to simulate interaction with a vector database
def vector_db_query(content):
    # This is a placeholder for actual vector database interactions.
    # In a real-world scenario, this would involve vectorizing the content and querying a vector DB.
    print("Querying vector database (simulated).")
    return None

# Example usage
if __name__ == "__main__":
    initialize_database()
    url = 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'
    handle_workflow(url)

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments

# Load pre-trained model and tokenizer
model_name = "facebook/bart-large-cnn"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define LoRA configuration
lora_config = {
    "r": 8,  # Rank of the adaptation matrix
    "alpha": 16,  # Scaling factor
    "dropout": 0.1  # Dropout rate for regularization
}

# Fine-tune model with LoRA
training_args = TrainingArguments(
    output_dir="./lora_bart",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=...,  # Your dataset here
    eval_dataset=...,  # Your dataset here
)

trainer.train()

import time

def benchmark(model, tokenizer, text):
    start_time = time.time()
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model.generate(**inputs)
    end_time = time.time()
    inference_time = end_time - start_time
    print(f"Inference Time: {inference_time:.4f} seconds")
    print(f"Model Size: {model.num_parameters()} parameters")

# Benchmark original model
benchmark(model, tokenizer, "Example text for benchmarking the model.")

initialize_database()
