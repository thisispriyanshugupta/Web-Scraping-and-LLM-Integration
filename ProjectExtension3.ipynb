{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlIOBR-nOL15",
        "outputId": "d3db9a6a-8420-4eab-ebc2-ba4156919afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3<3,>=1.21.1->requests) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 transformers selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from transformers import pipeline\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "\n",
        "# Setup ChromeDriver options for Colab\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Path to ChromeDriver executable compatible with Colab\n",
        "chrome_driver_path = '/usr/lib/chromium-browser/chromedriver'\n",
        "\n",
        "# Initialize ChromeDriver correctly\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Function to fetch web content using requests and BeautifulSoup\n",
        "def fetch_web_content(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "# Function to scrape content using BeautifulSoup\n",
        "def scrape_content(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    content = ' '.join([para.get_text() for para in paragraphs])\n",
        "    return content\n",
        "\n",
        "# Function to take screenshot using Selenium\n",
        "def take_screenshot(url, filename):\n",
        "    driver.get(url)\n",
        "    driver.save_screenshot(filename)\n",
        "\n",
        "# Function to generate content summary using Hugging Face's pipeline\n",
        "def generate_summary(content):\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summary = summarizer(content, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Function to initialize SQLite database\n",
        "def initialize_database():\n",
        "    conn = sqlite3.connect('workflow_results.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS workflow_results (\n",
        "            url TEXT PRIMARY KEY,\n",
        "            scraped_content TEXT,\n",
        "            llm_summary TEXT,\n",
        "            screenshot_path TEXT\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Function to store results in SQLite database\n",
        "def store_results(url, scraped_content, llm_summary, screenshot_path):\n",
        "    conn = sqlite3.connect('workflow_results.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        INSERT OR REPLACE INTO workflow_results (url, scraped_content, llm_summary, screenshot_path)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    ''', (url, scraped_content, llm_summary, screenshot_path))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "# Function to fetch cached results from SQLite database\n",
        "def fetch_cached_results(url):\n",
        "    conn = sqlite3.connect('workflow_results.db')\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        SELECT scraped_content, llm_summary, screenshot_path FROM workflow_results WHERE url=?\n",
        "    ''', (url,))\n",
        "    row = cursor.fetchone()\n",
        "    conn.close()\n",
        "    return row\n",
        "\n",
        "def handle_workflow(url):\n",
        "    # Check if results are cached\n",
        "    cached_results = fetch_cached_results(url)\n",
        "    if cached_results:\n",
        "        scraped_content, llm_summary, screenshot_path = cached_results\n",
        "        print(\"Results fetched from cache:\")\n",
        "    else:\n",
        "        # Fetch web content and scrape using BeautifulSoup\n",
        "        html_content = fetch_web_content(url)\n",
        "        scraped_content = scrape_content(html_content)[:1024]  # Truncate to 1024 tokens\n",
        "\n",
        "        # Take screenshot\n",
        "        screenshot_filename = 'screenshot.png'\n",
        "        take_screenshot(url, screenshot_filename)\n",
        "        screenshot_path = os.path.abspath(screenshot_filename)\n",
        "\n",
        "        # Generate LLM summary\n",
        "        llm_summary = generate_summary(scraped_content)\n",
        "\n",
        "        # Store results in database\n",
        "        store_results(url, scraped_content, llm_summary, screenshot_path)\n",
        "        print(\"Results generated and stored in database:\")\n",
        "\n",
        "    # Print results or further process as required\n",
        "    print(f\"Scraped Content:\\n{scraped_content}\\n\")\n",
        "    print(f\"LLM Generated Summary:\\n{llm_summary}\\n\")\n",
        "    print(f\"Screenshot Path: {screenshot_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    url = 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'\n",
        "    handle_workflow(url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCRG1lESOask",
        "outputId": "610a76da-2662-4e60-aea5-3159501240c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results generated and stored in database:\n",
            "Scraped Content:\n",
            "\n",
            " Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is artificial intelligence capable of generating text, images, videos, or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]\n",
            " Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA, text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney and DALL-E, and text-to-video AI generators such as Sora.[7][8][9][10] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][11][12]\n",
            " Generative AI has uses across a wide range of industries, including software development, healthcare, f\n",
            "\n",
            "LLM Generated Summary:\n",
            " Generative artificial intelligence (generative AI) is artificial intelligence capable of generating text, images, videos, or other data using generative models . Generative AI models learn patterns and structure of their input training data and then generate new data that has similar characteristics . Improvements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s . These include chatbots such as ChatGPT, Copilot, Gemini and LLaMA .\n",
            "\n",
            "Screenshot Path: /content/screenshot.png\n"
          ]
        }
      ]
    }
  ]
}